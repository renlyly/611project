{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251b577b-0667-4cb8-b04f-5112750a3f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Consider the following\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import hashlib\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Set of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "CACHE_DIR = \"page_cache\"\n",
    "\n",
    "# Create the cache directory if it doesn't exist\n",
    "if not os.path.exists(CACHE_DIR):\n",
    "    os.makedirs(CACHE_DIR)\n",
    "\n",
    "def md5_hash(url):\n",
    "    \"\"\"Returns the MD5 hash of a given URL.\"\"\"\n",
    "    return hashlib.md5(url.encode()).hexdigest()\n",
    "\n",
    "def cache_path(url):\n",
    "    \"\"\"Returns the cache file path for a given URL.\"\"\"\n",
    "    return os.path.join(CACHE_DIR, md5_hash(url))\n",
    "\n",
    "def fetch_raw(url):\n",
    "    \"\"\"Fetches the page content from the web without caching.\"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"curl/7.68.0\",  # Mimic the curl request\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\"\n",
    "    }\n",
    "    \n",
    "    # Print the current time in HH:MM:SS AM/PM format before each fetch\n",
    "    print(f\"Fetching {url} at {time.strftime('%I:%M:%S %p')}\")\n",
    "\n",
    "    try:\n",
    "        time.sleep(random.uniform(6, 12))  # Random delay to avoid hammering the server\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        else:\n",
    "            print(f\"Failed to fetch {url} with status code {response.status_code}\")\n",
    "            return None\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def fetch(url):\n",
    "    \"\"\"Wrapper function that implements caching around the raw fetch.\"\"\"\n",
    "    cache_file = cache_path(url)\n",
    "    \n",
    "    # Check if the page is cached\n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, 'r', encoding='utf-8') as file:\n",
    "            print(f\"Loading cached page for {url}\")\n",
    "            return BS(file.read(), \"html.parser\")\n",
    "    \n",
    "    # If not cached, fetch the raw page and cache it\n",
    "    page_content = fetch_raw(url)\n",
    "    if page_content:\n",
    "        with open(cache_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(page_content)\n",
    "        return BS(page_content, \"html.parser\")\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def episode_list_urls():\n",
    "    \"\"\"Fetches URLs of episodes from the main episode list page.\"\"\"\n",
    "    source_url = \"http://www.chakoteya.net/NextGen/episodes.htm\"\n",
    "    bs = fetch(source_url)\n",
    "    urls = []\n",
    "    for tb in bs.find_all(\"tbody\"):\n",
    "        for anchor in tb.find_all(\"a\"):\n",
    "            urls.append(f\"http://www.chakoteya.net/NextGen/{anchor.attrs['href']}\")\n",
    "    return urls\n",
    "\n",
    "def tokenize_and_count(text):\n",
    "    \"\"\"Tokenizes the text, removes punctuation, stopwords, downcases, and counts word frequencies.\"\"\"\n",
    "    # Remove punctuation and downcase\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation)).lower()\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Count the word frequencies\n",
    "    word_counts = Counter(filtered_tokens)\n",
    "\n",
    "    return word_counts\n",
    "\n",
    "def get_text_of_episodes():\n",
    "    \"\"\"Fetches and returns an array of objects with episode URLs and their text.\"\"\"\n",
    "    urls = episode_list_urls()\n",
    "    episodes = []\n",
    "\n",
    "    for url in urls:\n",
    "        bs = fetch(url)\n",
    "        b = bs.find(\"tbody\")\n",
    "        txt = b.text\n",
    "\n",
    "        # Store the URL and text in an object (dictionary) for each episode\n",
    "        episodes.append({\n",
    "            \"url\": url,\n",
    "            \"text\": txt\n",
    "        })\n",
    "\n",
    "    return episodes\n",
    "\n",
    "def get_word_counts_for_episodes(episodes):\n",
    "    \"\"\"Takes an array of episode objects and calculates word frequencies for each.\"\"\"\n",
    "    episode_word_counts = {}\n",
    "\n",
    "    for episode in episodes:\n",
    "        url = episode[\"url\"]\n",
    "        text = episode[\"text\"]\n",
    "\n",
    "        # Tokenize the text and count word frequencies\n",
    "        word_counts = tokenize_and_count(text)\n",
    "\n",
    "        # Store the word counts for each episode\n",
    "        episode_word_counts[url] = word_counts\n",
    "\n",
    "    return episode_word_counts\n",
    "\n",
    "def get_total_word_count(episode_word_counts):\n",
    "    \"\"\"Calculates the total word count across all episodes.\"\"\"\n",
    "    total_word_count = Counter()\n",
    "\n",
    "    for word_counts in episode_word_counts.values():\n",
    "        total_word_count.update(word_counts)\n",
    "\n",
    "    return total_word_count\n",
    "\n",
    "def convert_to_word_count_vectors(episode_word_counts, filtered_words):\n",
    "    \"\"\"Converts word counts for each episode into a vector following the filtered word order.\"\"\"\n",
    "    word_vectors = {}\n",
    "\n",
    "    for url, word_counts in episode_word_counts.items():\n",
    "        # Create a vector for this episode by the order of filtered_words\n",
    "        vector = [word_counts.get(word, 0) for word in filtered_words]\n",
    "        word_vectors[url] = vector\n",
    "\n",
    "    return word_vectors\n",
    "\n",
    "def write_word_counts_to_csv(word_count_vectors, filtered_words, filename=\"episode_word_counts.csv\"):\n",
    "    \"\"\"Writes the episode word count vectors to a CSV file.\"\"\"\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "\n",
    "        # Write the header row (episode URL and each word)\n",
    "        header = ['Episode URL'] + filtered_words\n",
    "        writer.writerow(header)\n",
    "\n",
    "        # Write each episode's word count vector\n",
    "        for url, vector in word_count_vectors.items():\n",
    "            row = [url] + vector\n",
    "            writer.writerow(row)\n",
    "\n",
    "# Example usage to fetch episode texts, get word counts, and calculate total word count\n",
    "episodes = get_text_of_episodes()\n",
    "episode_word_counts = get_word_counts_for_episodes(episodes)\n",
    "\n",
    "# Calculate total word count over all episodes\n",
    "total_word_count = get_total_word_count(episode_word_counts)\n",
    "\n",
    "# Filter words with a total count greater than 20 and sort by frequency\n",
    "filtered_words = [word for word, count in total_word_count.items() if count > 20]\n",
    "\n",
    "# Convert each episode's word counts into a vector of word counts\n",
    "word_count_vectors = convert_to_word_count_vectors(episode_word_counts, filtered_words)\n",
    "\n",
    "# Write the word count vectors to a CSV file\n",
    "write_word_counts_to_csv(word_count_vectors, filtered_words)\n",
    "\n",
    "print(f\"Word counts written to 'episode_word_counts.csv'\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
